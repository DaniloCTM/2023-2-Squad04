<header>
    Sprint 03 - Mar√© de Produtividade
</header>
<div class="doc-body">
<!-- ADD O CONTE√öDO ABAIXO -->

# üìú Introdu√ß√£o ao Scrapy e tutorias de instala√ß√£o

## O que √© o Scrapy? ü§î
O Scrapy √© um framework de web scraping de c√≥digo aberto em Python usado para extrair informa√ß√µes de sites da web de maneira automatizada. Web scraping √© o processo de coletar dados de p√°ginas da web de forma program√°tica, em vez de manualmente, o que √© √∫til para v√°rias finalidades, como coletar dados para an√°lise, pesquisa, minera√ß√£o de dados, entre outras.

O Scrapy fornece uma estrutura robusta e flex√≠vel para desenvolver [spiders](#nossa-primeira-spyder-üï∑Ô∏è), que s√£o programas que navegam na web, fazem solicita√ß√µes HTTP para p√°ginas da web, extraem dados de interesse e armazenam esses dados em um formato estruturado, como JSON, CSV ou em um banco de dados. Al√©m disso, o Scrapy lida com tarefas como gerenciamento de cookies, tratamento de redirecionamentos, paralelismo de solicita√ß√µes e muito mais.

**Clique** üëâ [**aqui**](https://github.com/unb-mds/2023-2-Squad04/blob/main/prototipos/tutorial_spyder/) **e tenha acesso ao nosso prot√≥tipo**

## Instala√ß√£o do Scrapy üë®‚Äçüîß 
Para come√ßar a usar o Scrapy, siga estas etapas para a instala√ß√£o: <br>
**‚ö†Ô∏è Certifique-se de que voc√™ tenha o Python instalado em seu sistema. O Scrapy requer Python 3.8+**
1. Verificando a vers√£o do python: `python --version`
2. Instalando o scrapy: 
 - Se voc√™ estiver usando Anaconda ou Miniconda, voc√™ pode instalar a partir do canal conda-forge, que possui pacotes atualizados para Linux, Windows e macOS: `conda install -c conda-forge scrapy`
 - Como alternativa, se voc√™ j√° estiver familiarizado com a instala√ß√£o de pacotes Python, voc√™ pode instalar o Scrapy e suas depend√™ncias do PyPI com: `pip install scrapy`
 - Verifique a instala√ß√£o: `scrapy --version` <br>
 
 3. ‚ö†Ô∏è **Em caso de erros, crie um Ambiente Virtual (Recomendado para evitar conflitos entre pacotes instalados no sistema):**
 - Primeiro, crie um ambiente virtual: `python3 -m venv meuambiente`
 - Ative o ambiente virtual: `source meuambiente/bin/activate`
 - Instale o scrapy: `pip install scrapy`
 - Para sair do ambiente virtual, basta digitar o seguinte comando: `deactivate`


## Exemplo de utiliza√ß√£o do Scrapy üßê
Vamos raspar [quotes.toscrape.com](quotes.toscrape.com), um site que lista cita√ß√µes de autores famosos.

### Nossa primeira Spyder üï∑Ô∏è

Spiders s√£o classes que voc√™ define e que o Scrapy usa para raspar informa√ß√µes de um site (ou de um grupo de sites).

#### Criando um projeto

Antes de come√ßar a fazer web scraping, voc√™ precisar√° configurar um novo projeto Scrapy. Acesse um diret√≥rio onde voc√™ deseja armazenar seu c√≥digo e execute:
`scrapy startproject tutorial`

Isso criar√° um diret√≥rio com o seguinte conte√∫do:
```
tutorial/
    scrapy.cfg            # arquivo de configura√ß√£o para implanta√ß√£o

    tutorial/             # m√≥dulo Python do projeto, voc√™ importar√° seu c√≥digo a partir daqui
        __init__.py

        items.py          # arquivo de defini√ß√£o de itens do projeto

        middlewares.py    # arquivo de middlewares do projeto

        pipelines.py      # arquivo de pipelines do projeto

        settings.py       # arquivo de configura√ß√µes do projeto

        spiders/          # um diret√≥rio onde voc√™ colocar√° suas spiders posteriormente
            __init__.py

```

Este √© o c√≥digo para a nossa primeira Spider.

```python
from pathlib import Path

import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            "https://quotes.toscrape.com/page/1/",
            "https://quotes.toscrape.com/page/2/",
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = f"quotes-{page}.html"
        Path(filename).write_bytes(response.body)
        self.log(f"Saved file {filename}")
```

Como voc√™ pode ver, nossa Spider √© uma subclasse de scrapy.Spider e define alguns atributos e m√©todos:

- **name:** identifica a Spider. Ele deve ser exclusivo dentro de um projeto, ou seja, voc√™ n√£o pode definir o mesmo nome para diferentes Spiders.

- **start_requests():** deve retornar um iter√°vel de Requests (voc√™ pode retornar uma lista de solicita√ß√µes ou escrever uma fun√ß√£o geradora) a partir das quais a Spider come√ßar√° a rastrear. Solicita√ß√µes subsequentes ser√£o geradas sucessivamente a partir dessas solicita√ß√µes iniciais.

- **parse():** um m√©todo que ser√° chamado para lidar com a resposta baixada para cada uma das solicita√ß√µes feitas. O par√¢metro de resposta √© uma inst√¢ncia de TextResponse que cont√©m o conte√∫do da p√°gina e possui m√©todos adicionais √∫teis para lidar com ele.

- O m√©todo **parse()** geralmente analisa a resposta, extrai os dados raspados como dicion√°rios e tamb√©m encontra novas URLs para seguir, criando novas solicita√ß√µes a partir delas.

#### Como executar nossa spyder üíª
Para colocar nossa spider para trabalhar, v√° para o diret√≥rio de n√≠vel superior do projeto e execute:<br>
`scrapy crawl quotes`

Este comando executa a spider com o nome que acabamos de adicionar, que enviar√° algumas solicita√ß√µes para o dom√≠nio. Voc√™ receber√° uma sa√≠da semelhante a esta:

```
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

Agora, verifique os arquivos no diret√≥rio atual. Voc√™ deve notar que dois novos arquivos foram criados: **quotes-1.html** e **quotes-2.html**, com o conte√∫do correspondente √†s respectivas URLs, conforme nosso m√©todo instrui.

#### O que aconteceu nos bastidores? ü§î

O Scrapy agendou os objetos retornados pelo m√©todo da Spider. Ao receber uma resposta para cada um deles, ele cria objetos de Resposta e chama o m√©todo de retorno de chamada associado √† solicita√ß√£o (neste caso, o m√©todo "parse"), passando a resposta como argumento.

#### Raspando apenas os dados e salvando em um .json üíæ
O c√≥digo abaixo √© um exemplo de uma spider que raspa dados das p√°ginas, em vez de salvar o HTML como fazia a do exemplo anterior.

**Clique** üëâ [**aqui**](https://github.com/unb-mds/2023-2-Squad04/blob/main/prototipos/tutorial_spyder/) **e tenha acesso ao nosso prot√≥tipo**

```
import scrapy

class QuotesSpider(scrapy.Spider):
    # Nome do spider
    name = "quotes"

    # URLs iniciais a serem raspadas
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    # Fun√ß√£o que √© chamada para processar cada p√°gina
    def parse(self, response):
        # Loop atrav√©s de cada elemento 'div.quote' na p√°gina
        for quote in response.css("div.quote"):
            # Extrai o texto da cita√ß√£o usando o seletor CSS
            text = quote.css("span.text::text").get()
            # Extrai o autor da cita√ß√£o usando o seletor CSS
            author = quote.css("span small::text").get()
            # Extrai todas as tags da cita√ß√£o usando o seletor CSS
            tags = quote.css("div.tags a.tag::text").getall()

            # Cria um dicion√°rio com os dados extra√≠dos
            quote_data = {
                "text": text,
                "author": author,
                "tags": tags,
            }

            # Retorna o dicion√°rio como resultado do processamento da p√°gina
            yield quote_data

        # Verifica se h√° uma pr√≥xima p√°gina
        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            # Se houver uma pr√≥xima p√°gina, segue o link e chama 'parse' novamente
            yield response.follow(next_page, self.parse)

```
Voc√™ pode armazenar os dados extra√≠dos em um formato de sua escolha, como JSON, CSV ou em um banco de dados. Para salvar os dados em um arquivo JSON, voc√™ pode usar o seguinte comando: `scrapy crawl livros -o livros.json`

# üïµÔ∏è Para saber mais...
Esse tutorial se baseou na documenta√ß√£o oficial do scrapy, se voc√™ quiser saber mais a fundo sobre todas as funcionalidades dispon√≠veis da ferramenta, clique üëâ [aqui](https://docs.scrapy.org/en/latest/index.html) e acesse a documenta√ß√£o completa.

<!-- ADD O CONTE√öDO ACIMA -->
</div>